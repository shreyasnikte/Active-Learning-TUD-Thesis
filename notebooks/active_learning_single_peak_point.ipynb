{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the simple active learning model of prediction of peak load shaving. The *ActivLearner* class is tweaked to get single data point of peak consumption value per chosen day instead of 48 values of a day. Most of the other functionality of the code is same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from math import sqrt\n",
    "import xgboost as xgb\n",
    "\n",
    "# For data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.models import Title\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "from tqdm import tqdm            #for .py version\n",
    "# from tqdm import tqdm_notebook as tqdm     # for .ipynb version\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dict 'params' consists of all the parameters used in the simulation software for ease of alteration\n",
    "params = {\n",
    "#         Set the regression model related parameters\n",
    "          'train_start_dt':'2013-01',\n",
    "          'train_stop_dt':'2013-12',\n",
    "          'y_variable': 'trial_d',\n",
    "          'X_variables':['trial_n', 'low', 'normal', 'high', 'WIND_DIRECTION', \n",
    "                         'WIND_SPEED', 'VISIBILITY', 'MSL_PRESSURE',\n",
    "                         'AIR_TEMPERATURE', 'DEWPOINT', 'WETB_TEMP', \n",
    "                         'STN_PRES', 'WMO_HR_SUN_DUR', 'hour', 'day'],\n",
    "    \n",
    "#         Set XGBoost regression parameters (for consumption model)\n",
    "          'n_estimators': 2000,\n",
    "          'early_stopping_rounds': 50,  #stop if 50 consequent rounds without decrease of error\n",
    "          'verbose': False,             # Change verbose to True if you want to see it train\n",
    "          'nthread': 4,\n",
    "    \n",
    "#         Set simulator parameters to default values\n",
    "          'season': 3,\n",
    "          'day_of_week': 3,\n",
    "          'special_event': 0,\n",
    "          'tariff_policy':[],\n",
    "    \n",
    "#         Set Occupant behaviour dynamics\n",
    "          'active_users': 0.1,#.5,     # Set the % of users who are willing to engage in the experiments\n",
    "          'avail_users': 0.1,#.5,       # Set the % of users who will be available to participate in specific experiment\n",
    "          'user_latency': 0,         # Set the values which correspond to real life participation delay for users \n",
    "          'frac_users_exp':1,      # Fraction of users selected for a particular trial\n",
    "          \n",
    "#         Set parameters for active learning\n",
    "          'total_experiments':100,#100, #Total number of experiments allowed per trial\n",
    "          'init_samples': 10,#50,      # Set the initial random samples to be chosen\n",
    "          'test_size':.3,           # Set test data size for splitting data in train-test\n",
    "          'X_var_activeL':['expected', 'dow', 'season'],\n",
    "          'y_var_activeL':'response'\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsumptionModel(object):\n",
    "    def __init__(self, df, params):\n",
    "        self.df = df\n",
    "        self.params = params\n",
    "#         some variables\n",
    "\n",
    "    def prep_data(self):\n",
    "        self.df = self.df.dropna().copy()\n",
    "        one_hot= pd.get_dummies(self.df['tariff'])\n",
    "        one_hot_renamed = one_hot.rename(index=str, columns={0.0399:'low', 0.1176:'normal', 0.672:'high'}) \n",
    "        self.df = self.df.join(one_hot_renamed).drop('tariff', axis=1)\n",
    "        \n",
    "        self.df[\"hour\"] = self.df.index.hour\n",
    "        self.df[\"day\"] = self.df.index.day\n",
    "        self.df[\"month\"] = self.df.index.month\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "#         Complete the xgboost model on 2013 data\n",
    "        self.X_train = self.df.loc[self.params[\"train_start_dt\"]:self.params[\"train_stop_dt\"],self.params[\"X_variables\"]]\n",
    "        self.y_train = self.df.loc[self.params[\"train_start_dt\"]:self.params[\"train_stop_dt\"],self.params[\"y_variable\"]]\n",
    "        self.X_test = self.df.loc[self.params[\"train_stop_dt\"]:,self.params[\"X_variables\"]]\n",
    "        self.y_test = self.df.loc[self.params[\"train_stop_dt\"]:,self.params[\"y_variable\"]]\n",
    "\n",
    "        self.xg_reg = xgb.XGBRegressor(n_estimators=self.params['n_estimators'], nthread = self.params[\"nthread\"])\n",
    "        self.xg_reg.fit(self.X_train, self.y_train,\n",
    "                        eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n",
    "                        early_stopping_rounds = self.params[\"early_stopping_rounds\"],\n",
    "                        verbose = self.params[\"verbose\"])\n",
    "\n",
    "#         Get feature importance chart\n",
    "        return xgb.plot_importance(self.xg_reg, height=0.9) # Plot feature importance\n",
    "      \n",
    "\n",
    "    def test(self, X_test, tariff):\n",
    "#         test the data points. Get the predictions\n",
    "#         self.preds = self.xg_reg.predict(X_test)\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def entropy(self):\n",
    "#         get entropy of each data point nad return the entropy dataframe\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulator:\n",
    "    \n",
    "    def __init__(self, df, df_weather, params):\n",
    "        self.params = params\n",
    "        self.df = df\n",
    "        self.df_weather = df_weather\n",
    "        active_users = int(len(df.columns)*self.params[\"active_users\"])   # get no. of active users from input percentage\n",
    "        self.active_users = random.sample(list(df.columns), active_users)\n",
    "        self.noisy_tariff = {}\n",
    "        self.spring = [3, 4, 5]\n",
    "        self.summer = [6, 7, 8]\n",
    "        self.autumn = [9, 10, 11]\n",
    "        self.winter = [1, 2, 12]\n",
    "\n",
    "\n",
    "    def select_day(self):\n",
    "#         Get user ids of participating users\n",
    "        self.fuzzy_participation()\n",
    "        \n",
    "#         Select the season\n",
    "        if selectsample.params[\"season\"] == -1:\n",
    "            month = random.randrange(1,12)\n",
    "        elif selectsample.params[\"season\"] == 0:\n",
    "            month = random.choice(self.spring)\n",
    "        elif selectsample.params[\"season\"] == 1:\n",
    "            month = random.choice(self.summer)\n",
    "        elif selectsample.params[\"season\"] == 2:\n",
    "            month = random.choice(self.autumn)\n",
    "        elif selectsample.params[\"season\"] == 3:\n",
    "            month = random.choice(self.winter)\n",
    "            \n",
    "#         Select the day of week\n",
    "        if selectsample.params[\"day_of_week\"] == -1:\n",
    "#             Select random day\n",
    "            dow = random.randrange(0,7)\n",
    "        else:\n",
    "            dow = selectsample.params[\"day_of_week\"] \n",
    "            \n",
    "#         Select the random day from the entries which satisfy above conditions\n",
    "        shortlist = self.df.loc[(self.df.index.month == month) & (self.df.index.dayofweek == dow), :].index\n",
    "        day = random.choice(shortlist.day.values)\n",
    "        year = random.choice(shortlist.year.values)\n",
    "        timestamp = str(year)+\"-\"+str(month)+\"-\"+str(day)\n",
    "        self.sample = self.df.loc[timestamp,self.avail_users]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def random_day(self):\n",
    "#         Get user ids of participating users\n",
    "        self.fuzzy_participation()\n",
    "    \n",
    "#         Sample a random day timestamp\n",
    "        shortlist = self.df.sample(axis = 0).index\n",
    "        day = random.choice(shortlist.day.values)\n",
    "        month = random.choice(shortlist.month.values)\n",
    "        year = random.choice(shortlist.year.values)\n",
    "        timestamp = str(year)+\"-\"+str(month)+\"-\"+str(day)\n",
    "        self.sample = self.df.loc[timestamp,self.avail_users]\n",
    "        \n",
    "        \n",
    "    \n",
    "    def fuzzy_participation(self):\n",
    "        avail_users = int(len(self.active_users)*self.params[\"avail_users\"])\n",
    "        self.avail_users = random.sample(self.active_users, avail_users)\n",
    "    \n",
    "    \n",
    "    def auto_noise_addition(self, levels, constraints):\n",
    "#         select the random users and their behaviour with random latency\n",
    "        self.noisy_tariff[\"h1_start\"] = [random.choice(range(constraints[\"h1_start\"]-2, \n",
    "                                                             constraints[\"h1_start\"]+int(trials_.duration/2))) for _ in range(len(self.avail_users))]\n",
    "        self.noisy_tariff[\"h1_end\"] = [random.choice(range(constraints[\"h1_end\"]-int(trials_.duration/2), \n",
    "                                                           constraints[\"h1_end\"]+2)) for _ in range(len(self.avail_users))]\n",
    "    \n",
    "\n",
    "    def tariff_policy(self, levels, constraints):\n",
    "#         use variables from auto_noise_addition and input variables of this function to create a tariff policy \n",
    "#         for each participating user **Needs more attention\n",
    "        self.auto_noise_addition(levels,constraints)\n",
    "    \n",
    "        self.d = np.ones((48, len(self.avail_users)))\n",
    "        self.df_tariff = pd.DataFrame(data=self.d, columns = self.avail_users)\n",
    "        for i in range(len(self.avail_users)):\n",
    "            self.df_tariff.loc[self.noisy_tariff[\"h1_start\"][i]:self.noisy_tariff[\"h1_end\"][i], self.avail_users[i]] = 2\n",
    "\n",
    "        self.df_tariff.index = self.sample.index\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "#         FOR EACH USER, call test function of consumption model, get modified behaviour, return original data point and modified data point\n",
    "        self.sample = self.sample.interpolate(method = 'linear', axis = 0).ffill().bfill()\n",
    "        self.sample = self.sample.join(self.df_weather.loc[self.sample.index,:])\n",
    "        df_response = pd.DataFrame()\n",
    "        self.sample[\"hour\"] = self.sample.index.hour\n",
    "        self.sample[\"day\"] = self.sample.index.day\n",
    "        self.sample[\"month\"] = self.sample.index.month\n",
    "        \n",
    "        list_ = [i for i in range(len(self.avail_users))]\n",
    "\n",
    "        for i in list_:\n",
    "            one_hot= pd.get_dummies(self.df_tariff[self.avail_users[i]])\n",
    "            one_hot_renamed = one_hot.rename(index=str, columns={1.0:'normal', 2.0:'high', 3.0:'low'}) \n",
    "            self.sample = pd.concat([self.sample, one_hot_renamed], axis =1)\n",
    "            self.sample[\"low\"] = 0\n",
    "\n",
    "            self.sample[\"trial_n\"] = self.sample[self.avail_users[i]]\n",
    "            \n",
    "#             consumption_model.test(self.sample[self.params['X_variables']], one_hot_renamed)\n",
    "            self.test()\n",
    "#             df_response[self.avail_users[i]] = consumption_model.preds\n",
    "            df_response[self.avail_users[i]] = self.preds\n",
    "            self.sample = self.sample.drop(['low', 'normal', 'high', 'trial_n'], axis= 1)\n",
    "            \n",
    "        df_response['response']= df_response.mean(axis = 1)\n",
    "        return df_response['response']\n",
    "            \n",
    "            \n",
    "            \n",
    "    def test(self):\n",
    "        self.preds = self.sample['trial_n']\n",
    "        self.preds.loc[self.sample['high']==1] = self.preds.loc[self.sample['high']==1]*0.9 #(1 - 9/(100*self.params['active_users']*self.params['avail_users']))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data():\n",
    "    try:\n",
    "        print(\"Reading aggregate consumption data...\")\n",
    "        df=pd.read_csv('~/Documents/work/Active-Learning-TUD-Thesis/mod_datasets/aggregate_consumption.csv', sep=',', header=0, index_col=0, parse_dates=['GMT'], low_memory=False)\n",
    "        df = df.drop_duplicates()\n",
    "        print(\"Done\")\n",
    "        print(\"Reading weather data...\")\n",
    "        df_midas=pd.read_csv('~/Documents/work/Active-Learning-TUD-Thesis/mod_datasets/midas_weather.csv', sep=',', header=0, index_col=0, parse_dates=['GMT'], low_memory=False)\n",
    "        df_midas_rs = df_midas.resample('30T').mean()\n",
    "        df_interpolated = df_midas_rs.interpolate(method='linear')\n",
    "        df_weather = df_interpolated.loc['2013-01':'2013-12',:]\n",
    "        df_final = pd.concat([df,df_weather], axis=1)\n",
    "        print(\"Done\")\n",
    "        print(\"Reading LCL consumption data...\")\n",
    "        df_n=pd.read_csv('~/Documents/work/Active-Learning-TUD-Thesis/UKDA-7857-csv/csv/data_collection/data_tables/consumption_n.csv', sep=',', header=0, index_col=0, parse_dates=['GMT'], low_memory=False)\n",
    "        df_n = df_n.drop_duplicates()\n",
    "        df_weath = df_interpolated.copy()\n",
    "        print(\"Done\")\n",
    "        \n",
    "    except Exception as e: print(e)\n",
    "        \n",
    "    return df_final, df_n, df_weath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class activeLearner(object):\n",
    "    \n",
    "    def __init__(self, df_n, df_weath, params):\n",
    "        self.df_n = df_n\n",
    "        self.df_weath = df_weath\n",
    "        self.params = params\n",
    "        \n",
    "    def run(self):\n",
    "        self.df_al = pd.DataFrame(columns = ['expected', 'response', 'dow', 'season'])\n",
    "        self.df_rand = pd.DataFrame(columns = ['expected', 'response', 'dow', 'season'])\n",
    "        self.get_random_samples()\n",
    "        mse_ActiveL = []\n",
    "        mse_Rand = []\n",
    "        \n",
    "        \n",
    "#         Create for loop to train the model for m number of times (where, m = self.params[\"total_experiments\"] - self.params[\"init_samples\"])\n",
    "        \n",
    "    \n",
    "        list_ = [i for i in range(self.params[\"total_experiments\"] - self.params[\"init_samples\"])]\n",
    "        for exp in tqdm(list_):\n",
    "        \n",
    "            # split data\n",
    "            X_train_AL, X_test_AL, y_train_AL, y_test_AL, X_train_rand, X_test_rand, y_train_rand, y_test_rand= self.split_data()\n",
    "            \n",
    "            # train the models\n",
    "            # model for Active learning\n",
    "            preds_ActiveL = self.run_random_forest_ActiveL(X_train_AL, X_test_AL, y_train_AL)\n",
    "            # model for Random sampling\n",
    "            preds_rand = self.run_random_forest_rand(X_train_rand, X_test_rand, y_train_rand)\n",
    "\n",
    "            \n",
    "            \n",
    "            # Query new data point\n",
    "            # New data point by selective sampling\n",
    "            sample_ActiveL, mse_activeL = selectsample.from_oracle(preds_ActiveL, X_test_AL, y_test_AL)\n",
    "            # New data point by random sampling\n",
    "            sample_rand, mse_rand = selectsample.from_oracle(preds_rand, X_test_rand, y_test_rand)\n",
    "\n",
    "            \n",
    "            # Add new data point to the existing data\n",
    "            # Dataset with selectively sampled data points\n",
    "            self.df_al = pd.concat([self.df_al, sample_ActiveL], axis=0, sort=True)\n",
    "            # Dataset of randomly sampled datapoints\n",
    "            self.df_rand = pd.concat([self.df_rand, sample_rand], axis=0, sort=True)\n",
    "\n",
    "            \n",
    "            mse_ActiveL.append(mse_activeL)\n",
    "            mse_Rand.append(mse_rand)\n",
    "            \n",
    "            # find entropy (optional)\n",
    "        return mse_ActiveL, mse_Rand\n",
    "            # for next experiment, get tariff policy, season and weekday \n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_random_samples(self):\n",
    "        temp_df = pd.DataFrame(columns = ['expected', 'response', 'dow', 'season'])\n",
    "        \n",
    "        \n",
    "#         select first random day of 48 data points\n",
    "        sim.random_day()\n",
    "        \n",
    "#         Add contextual data in future for the particular day to self.df\n",
    "        \n",
    "    \n",
    "#         Generate new tariff signals for one day\n",
    "        level, constraints = self.get_random_tariff()\n",
    "        \n",
    "#         Get schocastic behaviour of users\n",
    "        sim.tariff_policy(level, constraints)\n",
    "    \n",
    "    \n",
    "        response = sim.run()\n",
    "        response_max = response.max()     # Peak consumption as a response\n",
    "                \n",
    "        expected = sim.sample[sim.avail_users].mean(axis = 1).values\n",
    "        expected_max = expected.max()     # peak expected consumption\n",
    "        \n",
    "        dow, season = self.get_features()\n",
    "        \n",
    "        self.df_al.loc[0] = [expected_max, response_max, dow, season]\n",
    "\n",
    "            \n",
    "#        Create n number of datapoints from simulator (n=self.params[\"init_samples\"])\n",
    "#        Create a list of 1 to n to include a progress bar\n",
    "        \n",
    "        list_ = [i for i in range(self.params[\"init_samples\"])]\n",
    "\n",
    "        for i in tqdm(list_):\n",
    "            \n",
    "            sim.random_day()\n",
    "            # Add contextual data in future for the particular day to temp_df\n",
    "            level, constraints = self.get_random_tariff()\n",
    "            sim.tariff_policy(level, constraints)\n",
    "            \n",
    "            response = sim.run()\n",
    "            response_max = response.max()\n",
    "            \n",
    "            expected = sim.sample[sim.avail_users].mean(axis = 1).values\n",
    "            expected_max = expected.max()\n",
    "            \n",
    "            dow, season = self.get_features()\n",
    "            \n",
    "            temp_df.loc[0] = [expected_max, response_max, dow, season]\n",
    "            \n",
    "            self.df_al = pd.concat([self.df_al, temp_df], axis=0, sort=True)\n",
    "\n",
    "        self.df_rand = self.df_al.copy()  \n",
    "            \n",
    "            \n",
    "            \n",
    "    def split_data(self):\n",
    "        X_train_a, X_test_a, y_train_a, y_test_a = train_test_split(self.df_al[self.params['X_var_activeL']], \n",
    "                                                            self.df_al[self.params['y_var_activeL']], \n",
    "                                                            test_size= self.params[\"test_size\"])\n",
    "        \n",
    "        \n",
    "        X_train_rand, X_test_rand, y_train_rand, y_test_rand = train_test_split(self.df_rand[self.params['X_var_activeL']], \n",
    "                                                            self.df_rand[self.params['y_var_activeL']], \n",
    "                                                            test_size= self.params[\"test_size\"])\n",
    "        \n",
    "        return X_train_a, X_test_a, y_train_a, y_test_a, X_train_rand, X_test_rand, y_train_rand, y_test_rand \n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_features(self):\n",
    "        try:\n",
    "#             get day of week encoding\n",
    "            dow = sim.sample.index[0].dayofweek\n",
    "#             get month of year encoding\n",
    "            month = sim.sample.index[0].month\n",
    "#             we are more interested in season based behaviour than monthly behaviour\n",
    "            \n",
    "            if month in [3,4,5]:\n",
    "                season =0\n",
    "            elif month in [6,7,8]:\n",
    "                season = 1\n",
    "            elif month in [9,10,11]:\n",
    "                season = 2\n",
    "            elif month in [1,2,12]:\n",
    "                season = 3\n",
    "            \n",
    "        except Exception as e: print(e)    \n",
    "\n",
    "        return dow, season\n",
    "        \n",
    "    def run_random_forest_ActiveL(self, X_train, X_test, y_train):\n",
    "        self.regres = RandomForestRegressor(max_depth=2, \n",
    "                                                random_state=0, \n",
    "                                                n_estimators=100)\n",
    "        self.regres.fit(X_train, y_train)\n",
    "        test_y_predicted = self.regres.predict(X_test)\n",
    "        return (test_y_predicted)\n",
    "    \n",
    "    \n",
    "    def run_random_forest_rand(self, X_train, X_test, y_train):\n",
    "        self.regres = RandomForestRegressor(max_depth=2, \n",
    "                                                random_state=0, \n",
    "                                                n_estimators=100)\n",
    "        self.regres.fit(X_train, y_train)\n",
    "        test_y_predicted = self.regres.predict(X_test)\n",
    "        return (test_y_predicted)\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "    def get_random_tariff(self):\n",
    "        self.year = random.randrange(2012,2013)\n",
    "        self.month = random.randrange(1,12)\n",
    "        self.day = random.randrange(1,28)\n",
    "        self.hour = random.randrange(17,18)\n",
    "        self.minute = random.choice([0,30])\n",
    "        self.duration = random.randrange(6, 8)\n",
    "        index = datetime(self.year, self.month, self.day, self.hour, self.minute, 0)\n",
    "        h1_start = int(index.hour * 2) + int(index.minute / 30)\n",
    "        h1_end = h1_start + self.duration\n",
    "        constraints = {\"h1_start\": h1_start, \"h1_end\": h1_end}\n",
    "        level = 0       #dummy\n",
    "        return level, constraints\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectSample(object):\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        \n",
    "    def from_oracle(self, preds, X_test, y_test):\n",
    "        #Select the point with maximum error\n",
    "        df_y_test = y_test.reset_index()\n",
    "        d = {'preds': preds}\n",
    "        df_preds = pd.DataFrame(data = d)\n",
    "        df_X_test = X_test.reset_index()\n",
    "        \n",
    "        error_ = (df_y_test['response']-df_preds['preds'])**2\n",
    "        \n",
    "        mse = ((df_y_test['response']-df_preds['preds'])**2).mean(axis=0)\n",
    "        \n",
    "        self.params[\"day_of_week\"] = df_X_test.loc[error_.idxmax(),'dow']\n",
    "        self.params[\"season\"] = df_X_test.loc[error_.idxmax(),'season']\n",
    "        \n",
    "        # Generate new data point for above dow and season\n",
    "        \n",
    "        sim.select_day()\n",
    "        level, constraints = trials_.get_random_tariff()\n",
    "        sim.tariff_policy(level, constraints)\n",
    "            \n",
    "        response = sim.run()\n",
    "        response_max = response.max()\n",
    "          \n",
    "        expected = sim.sample[sim.avail_users].mean(axis = 1).values\n",
    "        expected_max = expected.max()\n",
    "          \n",
    "        dow, season = trials_.get_features()\n",
    "    \n",
    "        df = pd.DataFrame(columns = ['expected', 'response', 'dow', 'season'])\n",
    "        df.loc[0] = [expected_max, response_max, dow, season]\n",
    "        return df, mse\n",
    "        \n",
    "    def random(self):\n",
    "        #Randomly select next data point\n",
    "        sim.random_day()\n",
    "        level, constraints = trials_.get_random_tariff()\n",
    "        sim.tariff_policy(level, constraints)\n",
    "            \n",
    "        response = sim.run()\n",
    "        response_max = response.max()\n",
    "          \n",
    "        expected = sim.sample[sim.avail_users].mean(axis = 1).values\n",
    "        expected_max = expected.max()\n",
    "          \n",
    "        dow, season = trials_.get_features()\n",
    "    \n",
    "        df = pd.DataFrame(columns = ['expected', 'response', 'dow', 'season'])\n",
    "        df.loc[0] = [expected_max, response_max, dow, season]\n",
    "        return df, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init():\n",
    "    df_final, df_n, df_weath = import_data()\n",
    "    \n",
    "    try:\n",
    "        cons_model = ConsumptionModel(df_final, params)\n",
    "        sim = Simulator(df_n.loc['2012-05':, :], df_weath.loc['2012-05':, :], params)\n",
    "#         generate =  randomGenerate(params)\n",
    "        trials_ = activeLearner(df_n.loc['2012-05':, :], df_weath.loc['2012-05':, :], params)\n",
    "        selectsample = SelectSample(params)\n",
    "        \n",
    "    except Exception as e: print(e)    \n",
    "    \n",
    "    return cons_model, sim, trials_, selectsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bokeh(mse_ActiveL, mse_Rand, params):\n",
    "    output_notebook()\n",
    "#     output_file(\"./temp/line.html\") #Uncomment it to save the plot in html file\n",
    "    list_ = [i for i in range(params[\"total_experiments\"])]\n",
    "\n",
    "    p=figure(plot_width=800, plot_height=400)\n",
    "    p.line(list_, mse_ActiveL, line_width=1, color='blue')\n",
    "    p.line(list_, mse_Rand, line_width=1, color='red')\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading aggregate consumption data...\n",
      "Done\n",
      "Reading weather data...\n",
      "Done\n",
      "Reading LCL consumption data...\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 1/10 [00:00<00:05,  1.60it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 2/10 [00:01<00:04,  1.61it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 3/10 [00:01<00:04,  1.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 5/10 [00:03<00:03,  1.59it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 7/10 [00:04<00:01,  1.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 8/10 [00:05<00:01,  1.56it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 9/10 [00:05<00:00,  1.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 1/90 [00:01<02:12,  1.49s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 2/90 [00:02<02:09,  1.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 3/90 [00:04<02:07,  1.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 4/90 [00:05<02:05,  1.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 5/90 [00:07<02:02,  1.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 6/90 [00:08<02:00,  1.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 7/90 [00:10<02:01,  1.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 8/90 [00:11<02:03,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 9/90 [00:13<02:11,  1.63s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 10/90 [00:15<02:14,  1.68s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 11/90 [00:17<02:10,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 12/90 [00:18<02:07,  1.64s/it]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 13/90 [00:20<02:04,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 14/90 [00:21<01:59,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 15/90 [00:23<01:56,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 16/90 [00:24<01:53,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 17/90 [00:26<01:51,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 18/90 [00:27<01:49,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 19/90 [00:29<01:48,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 20/90 [00:30<01:46,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 21/90 [00:32<01:44,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 22/90 [00:33<01:41,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 23/90 [00:35<01:40,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 24/90 [00:36<01:38,  1.48s/it]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 25/90 [00:38<01:36,  1.48s/it]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 26/90 [00:39<01:34,  1.47s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 27/90 [00:41<01:34,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 28/90 [00:42<01:31,  1.48s/it]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 29/90 [00:44<01:29,  1.47s/it]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 30/90 [00:45<01:27,  1.46s/it]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 31/90 [00:46<01:26,  1.46s/it]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 32/90 [00:48<01:25,  1.47s/it]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 33/90 [00:49<01:24,  1.48s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 34/90 [00:51<01:24,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 35/90 [00:52<01:22,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 36/90 [00:54<01:19,  1.48s/it]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 37/90 [00:55<01:19,  1.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 38/90 [00:57<01:17,  1.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 39/90 [00:58<01:16,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 40/90 [01:00<01:15,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 41/90 [01:02<01:14,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 42/90 [01:03<01:12,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 43/90 [01:05<01:10,  1.50s/it]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 44/90 [01:06<01:08,  1.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 45/90 [01:08<01:07,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 46/90 [01:09<01:05,  1.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 47/90 [01:11<01:04,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 48/90 [01:12<01:02,  1.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 49/90 [01:13<01:01,  1.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 50/90 [01:15<00:59,  1.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 51/90 [01:16<00:57,  1.47s/it]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 52/90 [01:18<00:56,  1.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 53/90 [01:19<00:54,  1.46s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 54/90 [01:21<00:53,  1.49s/it]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 55/90 [01:23<00:54,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 56/90 [01:26<01:08,  2.01s/it]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 57/90 [01:27<01:01,  1.86s/it]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 58/90 [01:29<00:57,  1.81s/it]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 59/90 [01:30<00:54,  1.76s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 60/90 [01:32<00:51,  1.70s/it]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 61/90 [01:34<00:47,  1.64s/it]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 62/90 [01:35<00:44,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 63/90 [01:36<00:41,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 64/90 [01:38<00:39,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 65/90 [01:40<00:38,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 66/90 [01:41<00:36,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 67/90 [01:43<00:34,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 68/90 [01:44<00:33,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 69/90 [01:46<00:31,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 70/90 [01:48<00:33,  1.68s/it]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 71/90 [01:49<00:32,  1.72s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 72/90 [01:51<00:30,  1.68s/it]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 73/90 [01:53<00:29,  1.72s/it]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 74/90 [01:54<00:26,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 75/90 [01:56<00:24,  1.62s/it]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 76/90 [01:57<00:22,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 77/90 [01:59<00:20,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 78/90 [02:01<00:18,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 79/90 [02:02<00:17,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 80/90 [02:04<00:15,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 81/90 [02:05<00:13,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 82/90 [02:07<00:12,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 83/90 [02:08<00:10,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 84/90 [02:10<00:09,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 85/90 [02:11<00:07,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 86/90 [02:13<00:06,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 87/90 [02:14<00:04,  1.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 88/90 [02:17<00:03,  1.78s/it]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 89/90 [02:18<00:01,  1.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 90/90 [02:20<00:00,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 1/10 [00:00<00:05,  1.58it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 2/10 [00:01<00:05,  1.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 3/10 [00:01<00:04,  1.54it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 4/10 [00:02<00:03,  1.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 5/10 [00:03<00:03,  1.51it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 6/10 [00:03<00:02,  1.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 7/10 [00:04<00:01,  1.53it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 8/10 [00:05<00:01,  1.52it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 9/10 [00:05<00:00,  1.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 10/10 [00:06<00:00,  1.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 1/90 [00:01<02:21,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 2/90 [00:03<02:18,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 3/90 [00:05<02:36,  1.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 4/90 [00:06<02:27,  1.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 5/90 [00:08<02:19,  1.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 6/90 [00:09<02:14,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 7/90 [00:11<02:10,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 8/90 [00:13<02:26,  1.79s/it]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 9/90 [00:15<02:17,  1.69s/it]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 10/90 [00:16<02:11,  1.64s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 11/90 [00:18<02:06,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 12/90 [00:19<02:02,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 13/90 [00:21<02:10,  1.69s/it]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 14/90 [00:23<02:13,  1.76s/it]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 15/90 [00:25<02:06,  1.68s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 16/90 [00:26<02:02,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 17/90 [00:28<01:57,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 18/90 [00:29<01:52,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 19/90 [00:31<01:50,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 20/90 [00:32<01:50,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 21/90 [00:34<01:48,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 22/90 [00:35<01:45,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 23/90 [00:37<01:45,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 24/90 [00:39<01:43,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 25/90 [00:40<01:41,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 26/90 [00:42<01:38,  1.54s/it]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 27/90 [00:43<01:37,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 28/90 [00:45<01:34,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 29/90 [00:46<01:33,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 30/90 [00:48<01:39,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 31/90 [00:50<01:35,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 32/90 [00:51<01:30,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 33/90 [00:53<01:29,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 34/90 [00:54<01:26,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 35/90 [00:56<01:31,  1.67s/it]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 36/90 [00:58<01:29,  1.65s/it]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 37/90 [00:59<01:26,  1.63s/it]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 38/90 [01:01<01:22,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 39/90 [01:03<01:22,  1.62s/it]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 40/90 [01:04<01:20,  1.61s/it]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 41/90 [01:06<01:16,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 42/90 [01:07<01:14,  1.56s/it]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 43/90 [01:09<01:13,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▉     | 44/90 [01:10<01:11,  1.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 45/90 [01:12<01:08,  1.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 51%|█████     | 46/90 [01:13<01:07,  1.52s/it]\u001b[A\u001b[A\n",
      "\n",
      " 52%|█████▏    | 47/90 [01:15<01:08,  1.58s/it]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 48/90 [01:17<01:08,  1.63s/it]\u001b[A\u001b[A\n",
      "\n",
      " 54%|█████▍    | 49/90 [01:18<01:05,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 50/90 [01:20<01:02,  1.57s/it]\u001b[A\u001b[A\n",
      "\n",
      " 57%|█████▋    | 51/90 [01:22<01:04,  1.64s/it]\u001b[A\u001b[A\n",
      "\n",
      " 58%|█████▊    | 52/90 [01:23<01:02,  1.64s/it]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 53/90 [01:25<00:59,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 54/90 [01:26<00:57,  1.59s/it]\u001b[A\u001b[A\n",
      "\n",
      " 61%|██████    | 55/90 [01:28<00:57,  1.64s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 56/90 [01:32<01:22,  2.43s/it]\u001b[A\u001b[A\n",
      "\n",
      " 63%|██████▎   | 57/90 [01:34<01:15,  2.29s/it]\u001b[A\u001b[A\n",
      "\n",
      " 64%|██████▍   | 58/90 [01:36<01:06,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      " 66%|██████▌   | 59/90 [01:38<01:05,  2.11s/it]\u001b[A\u001b[A\n",
      "\n",
      " 67%|██████▋   | 60/90 [01:40<00:58,  1.96s/it]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 61/90 [01:41<00:54,  1.87s/it]\u001b[A\u001b[A\n",
      "\n",
      " 69%|██████▉   | 62/90 [01:44<00:55,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 63/90 [01:46<00:54,  2.03s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 64/90 [01:48<00:58,  2.24s/it]\u001b[A\u001b[A\n",
      "\n",
      " 72%|███████▏  | 65/90 [01:50<00:52,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      " 73%|███████▎  | 66/90 [01:52<00:47,  1.98s/it]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▍  | 67/90 [01:55<00:50,  2.18s/it]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▌  | 68/90 [01:56<00:44,  2.00s/it]\u001b[A\u001b[A\n",
      "\n",
      " 77%|███████▋  | 69/90 [01:58<00:39,  1.87s/it]\u001b[A\u001b[A\n",
      "\n",
      " 78%|███████▊  | 70/90 [01:59<00:36,  1.81s/it]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 71/90 [02:01<00:33,  1.77s/it]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 72/90 [02:03<00:31,  1.77s/it]\u001b[A\u001b[A\n",
      "\n",
      " 81%|████████  | 73/90 [02:04<00:28,  1.69s/it]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 74/90 [02:06<00:27,  1.73s/it]\u001b[A\u001b[A\n",
      "\n",
      " 83%|████████▎ | 75/90 [02:08<00:25,  1.69s/it]\u001b[A\u001b[A\n",
      "\n",
      " 84%|████████▍ | 76/90 [02:10<00:24,  1.75s/it]\u001b[A\u001b[A\n",
      "\n",
      " 86%|████████▌ | 77/90 [02:12<00:24,  1.91s/it]\u001b[A\u001b[A\n",
      "\n",
      " 87%|████████▋ | 78/90 [02:14<00:22,  1.85s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 79/90 [02:16<00:21,  1.93s/it]\u001b[A\u001b[A\n",
      "\n",
      " 89%|████████▉ | 80/90 [02:17<00:18,  1.87s/it]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 81/90 [02:19<00:16,  1.86s/it]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 82/90 [02:21<00:15,  1.96s/it]\u001b[A\u001b[A\n",
      "\n",
      " 92%|█████████▏| 83/90 [02:23<00:13,  1.96s/it]\u001b[A\u001b[A\n",
      "\n",
      " 93%|█████████▎| 84/90 [02:25<00:11,  1.92s/it]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 85/90 [02:27<00:09,  1.88s/it]\u001b[A\u001b[A\n",
      "\n",
      " 96%|█████████▌| 86/90 [02:30<00:08,  2.19s/it]\u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 87/90 [02:32<00:06,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      " 98%|█████████▊| 88/90 [02:34<00:04,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      " 99%|█████████▉| 89/90 [02:35<00:01,  1.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 90/90 [02:37<00:00,  1.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 1/10 [00:00<00:06,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 2/10 [00:01<00:05,  1.44it/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 3/10 [00:02<00:04,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 4/10 [00:02<00:04,  1.41it/s]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 5/10 [00:03<00:03,  1.35it/s]\u001b[A\u001b[A\n",
      "\n",
      " 60%|██████    | 6/10 [00:04<00:02,  1.34it/s]\u001b[A\u001b[A\n",
      "\n",
      " 70%|███████   | 7/10 [00:05<00:02,  1.36it/s]\u001b[A\u001b[A\n",
      "\n",
      " 80%|████████  | 8/10 [00:05<00:01,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      " 90%|█████████ | 9/10 [00:06<00:00,  1.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 10/10 [00:07<00:00,  1.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 0/90 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 1/90 [00:01<02:22,  1.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 2/90 [00:03<02:26,  1.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 3/90 [00:05<02:39,  1.84s/it]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 4/90 [00:07<02:34,  1.79s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 5/90 [00:08<02:27,  1.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 6/90 [00:12<03:03,  2.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 7/90 [00:14<03:05,  2.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 8/90 [00:16<02:49,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 9/90 [00:17<02:39,  1.97s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 48 elements, new values have 1 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-18b3118b2171>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mmse_ActiveL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_Rand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrials_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mmse_AL_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse_ActiveL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmse_Rand_total\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse_Rand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-72a7aee3500f>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# Query new data point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# New data point by selective sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0msample_ActiveL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_activeL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselectsample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_oracle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_ActiveL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_AL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_AL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;31m# New data point by random sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0msample_rand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_rand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselectsample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_oracle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_rand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_rand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_rand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-c2e22c84ec79>\u001b[0m in \u001b[0;36mfrom_oracle\u001b[0;34m(self, preds, X_test, y_test)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_day\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrials_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_random_tariff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtariff_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-6ba946a90286>\u001b[0m in \u001b[0;36mtariff_policy\u001b[0;34m(self, levels, constraints)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_tariff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoisy_tariff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"h1_start\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoisy_tariff\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"h1_end\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavail_users\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_tariff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5078\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5079\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5080\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5081\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5082\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    153\u001b[0m             raise ValueError(\n\u001b[1;32m    154\u001b[0m                 \u001b[0;34m'Length mismatch: Expected axis has {old} elements, new '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 'values have {new} elements'.format(old=old_len, new=new_len))\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 48 elements, new values have 1 elements"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     # import data and declare classes\n",
    "    cons_model, sim, trials_, selectsample= _init()\n",
    "\n",
    "#     # start the simulator and active learning by membership query synthesis\n",
    "\n",
    "    mse_ActiveL, mse_Rand = trials_.run()\n",
    "    list_ = [i for i in range(len(mse_ActiveL))]\n",
    "    d1 = {'0':mse_ActiveL}\n",
    "    mse_AL_total = pd.DataFrame(data=d1)\n",
    "    d2 = {'0':mse_Rand}\n",
    "    mse_Rand_total = pd.DataFrame(data=d2)\n",
    "    \n",
    "    for i in range(3):\n",
    "        mse_ActiveL, mse_Rand = trials_.run()\n",
    "        mse_AL_total.loc[:,str(i+1)] = mse_ActiveL\n",
    "        mse_Rand_total.loc[:, str(i+1)] = mse_Rand\n",
    "        \n",
    "    plot_bokeh(mse_AL_total.mean(axis=1), mse_Rand_total.mean(axis=1), params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # start the simulator and active learning by membership query synthesis\n",
    "    mse_ActiveL, mse_Rand = trials_.run()\n",
    "    list_ = [i for i in range(len(mse_ActiveL))]\n",
    "    d1 = {'0':mse_ActiveL}\n",
    "    mse_AL_total = pd.DataFrame(data=d1)\n",
    "    d2 = {'0':mse_Rand}\n",
    "    mse_Rand_total = pd.DataFrame(data=d2)\n",
    "    \n",
    "    for i in range(50):\n",
    "        mse_ActiveL, mse_Rand = trials_.run()\n",
    "        mse_AL_total.loc[:,str(i+1)] = mse_ActiveL\n",
    "        mse_Rand_total.loc[:, str(i+1)] = mse_Rand\n",
    "        \n",
    "    plot_bokeh(mse_AL_total.mean(axis=1), mse_Rand_total.mean(axis=1), params, list_)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i =1\n",
    "str(1+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate.experiment()\n",
    "# level, constraints = generate.tariff()\n",
    "# sim.tariff_policy(level, constraints)\n",
    "# response = sim.run()\n",
    "# response\n",
    "\n",
    "a = trials_.y_test.reset_index()\n",
    "d = {'preds': trials_.preds}\n",
    "b = pd.DataFrame(data = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df__ = pd.DataFrame(data=response, index = sim.sample.index)\n",
    "c = (b['preds'] - a['response'])**2\n",
    "c.idxmax()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
